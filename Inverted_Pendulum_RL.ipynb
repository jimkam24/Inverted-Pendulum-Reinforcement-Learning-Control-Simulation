{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ye-M2CTd2cIH"
      },
      "source": [
        "## Step 1: Physical System"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oAD2YmVIyxrI"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "# ========== Constants (from the paper) ========== #\n",
        "\n",
        "# Simulation parameter\n",
        "dt = 0.05  # s\n",
        "\n",
        "# Pendulum parameters\n",
        "omega = 4.882          # rad/s\n",
        "kv = 0.07              # viscous friction\n",
        "g = 9.81\n",
        "l = g / omega**2       # pendulum length\n",
        "\n",
        "# Cart / actuator parameters\n",
        "tau = 0.0482\n",
        "kU = 0.051\n",
        "fc = 1.166\n",
        "fd = -0.097\n",
        "\n",
        "# Noise parameter (measured from the physical system in the paper)\n",
        "sigma_theta = 0.0026     # rad (angle sensor noise)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMF-azOZ3PDi"
      },
      "outputs": [],
      "source": [
        "# ========== Class to simulate the physical system ========== #\n",
        "\n",
        "class CartPoleSimulator:\n",
        "    def __init__(self):\n",
        "        # True physical state\n",
        "        self.theta = np.pi      # upright position\n",
        "        self.theta_dot = 0.0\n",
        "        self.x = 0.0\n",
        "        self.x_dot = 0.0\n",
        "\n",
        "    def step(self, U):\n",
        "\n",
        "        # function to simulate one step as a function of the control voltage U\n",
        "        # it returns only noisy observations (check paper and report)\n",
        "\n",
        "        # ---------- Eq. (3): commanded velocity ---------- #\n",
        "        x_dot_c = kU * U\n",
        "\n",
        "        # ---------- Eq. (2): cart acceleration ---------- #\n",
        "        x_ddot = (\n",
        "            (x_dot_c - self.x_dot) / tau\n",
        "            - fc * np.sign(self.x_dot)\n",
        "            - fd\n",
        "        )\n",
        "\n",
        "        # ---------- Eq. (1): pendulum angular acceleration ---------- #\n",
        "        theta_ddot = (\n",
        "            - kv * self.theta_dot\n",
        "            - omega**2 * np.sin(self.theta)\n",
        "            - (x_ddot / l) * np.cos(self.theta)\n",
        "        )\n",
        "\n",
        "        # ---------- Integrate (semi-implicit Euler) ---------- #\n",
        "        self.theta_dot += theta_ddot * dt\n",
        "        self.theta += self.theta_dot * dt\n",
        "\n",
        "        self.x_dot += x_ddot * dt\n",
        "        self.x += self.x_dot * dt\n",
        "\n",
        "        # ---------- Measurement noise (as in the paper) ---------- #\n",
        "        # Noisy angle measurement\n",
        "        theta_m = self.theta + np.random.normal(0.0, sigma_theta)\n",
        "\n",
        "        # Induced angular velocity noise\n",
        "        sigma_theta_dot = sigma_theta / dt\n",
        "        theta_dot_m = self.theta_dot + np.random.normal(0.0, sigma_theta_dot)\n",
        "\n",
        "        # ---------- Observation ---------- #\n",
        "        # use sin and cos in the state and not θ (Why? -> paper - report)\n",
        "        observation = np.array([\n",
        "            np.sin(theta_m),\n",
        "            np.cos(theta_m),\n",
        "            theta_dot_m,\n",
        "            self.x,\n",
        "            self.x_dot\n",
        "        ])\n",
        "\n",
        "        return observation\n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "      # ----- True physical state -----\n",
        "      self.theta = np.pi          # upright position\n",
        "      self.theta_dot = 0.0\n",
        "      self.x = 0.0\n",
        "      self.x_dot = 0.0\n",
        "\n",
        "      # ----- Small random perturbations -----\n",
        "      # Helps exploration and avoids overfitting a single IC\n",
        "      self.theta += np.random.uniform(-0.05, 0.05)   # ~ +-3 degrees\n",
        "      self.x += np.random.uniform(-0.02, 0.02)\n",
        "\n",
        "      return self._make_observation()\n",
        "\n",
        "    def _make_observation(self):\n",
        "        # Noisy angle measurement\n",
        "        theta_m = self.theta + np.random.normal(0.0, sigma_theta)\n",
        "\n",
        "        # Noisy angular velocity measurement\n",
        "        theta_dot_m = self.theta_dot + np.random.normal(0.0, sigma_theta / dt)\n",
        "\n",
        "        obs = np.array([\n",
        "            np.sin(theta_m),\n",
        "            np.cos(theta_m),\n",
        "            theta_dot_m,\n",
        "            self.x,\n",
        "            self.x_dot\n",
        "        ], dtype=np.float32)\n",
        "\n",
        "        return obs\n",
        "    \n",
        "    def reset_downward(self):\n",
        "        self.theta = 0.0\n",
        "        self.theta_dot = 0.0\n",
        "        self.x = 0.0\n",
        "        self.x_dot = 0.0\n",
        "        return self._make_observation()\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqBlPGwC7bc4"
      },
      "source": [
        "## Step 2: Reward function, Episode Termination Logic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXtTXnvU7aey"
      },
      "outputs": [],
      "source": [
        "# ========== Constants (from the paper) ========== #\n",
        "\n",
        "# Episode / environment limits\n",
        "x_max = 0.35           # m\n",
        "theta_dot_max = 14.0   # rad/s\n",
        "max_steps = 800        # steps per episode (max real system time is 40s)\n",
        "\n",
        "# Reward scaling\n",
        "x0 = x_max"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G_-osrK_7kap"
      },
      "outputs": [],
      "source": [
        "# ========== reward function ========== #\n",
        "def compute_reward(theta, x):\n",
        "    return 0.5 * (1 - np.cos(theta)) - (x / x0)**2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nBbEA6Ne7qBF"
      },
      "outputs": [],
      "source": [
        "# ========== function to follow the paper's termination rules ========== #\n",
        "def check_termination(x, theta_dot, step_count):\n",
        "    if abs(x) > x_max:\n",
        "        return True, -400.0\n",
        "    if abs(theta_dot) > theta_dot_max:\n",
        "        return True, 0.0\n",
        "    if step_count >= max_steps:\n",
        "        return True, 0.0\n",
        "    return False, 0.0\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lSuc4by0_Ipy"
      },
      "source": [
        "## Step 3: Q-learning algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NrvssFIr_Lvx"
      },
      "outputs": [],
      "source": [
        "# ========== Constants check and move ========== #\n",
        "U0 = 12                 # [V] constant voltage magnitude\n",
        "\n",
        "actions = np.array([-U0, 0.0, U0])\n",
        "N_ACTIONS = len(actions)\n",
        "\n",
        "\n",
        "# ========== Q-learning hyperparameters (check paper file S1) ========== #\n",
        "\n",
        "alpha = 0.01      # learning rate\n",
        "gamma = 0.99      # discount factor\n",
        "epsilon_min = 0.1\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NbWAwhbs_Tix"
      },
      "outputs": [],
      "source": [
        "# ========== Discredization Process ========== #\n",
        "\n",
        "n_bins = {\n",
        "    \"sin_theta\": 10,\n",
        "    \"cos_theta\": 10,\n",
        "    \"theta_dot\": 10,\n",
        "    \"x\": 10,\n",
        "    \"x_dot\": 10\n",
        "}\n",
        "\n",
        "state_ranges = {\n",
        "    \"sin_theta\": (-1.0, 1.0),\n",
        "    \"cos_theta\": (-1.0, 1.0),\n",
        "    \"theta_dot\": (-14.0, 14.0),   # from termination condition\n",
        "    \"x\": (-0.35, 0.35),\n",
        "    \"x_dot\": (-2.0, 2.0)          # conservative bound\n",
        "}\n",
        "\n",
        "def discretize(value, min_val, max_val, n_bins):\n",
        "    value = np.clip(value, min_val, max_val)\n",
        "    bin_width = (max_val - min_val) / n_bins\n",
        "    idx = int((value - min_val) / bin_width)\n",
        "    return min(idx, n_bins - 1)\n",
        "\n",
        "\n",
        "\n",
        "def discretize_state(obs):\n",
        "    sin_t, cos_t, theta_dot, x, x_dot = obs\n",
        "\n",
        "    return (\n",
        "        discretize(sin_t, *state_ranges[\"sin_theta\"], n_bins[\"sin_theta\"]),\n",
        "        discretize(cos_t, *state_ranges[\"cos_theta\"], n_bins[\"cos_theta\"]),\n",
        "        discretize(theta_dot, *state_ranges[\"theta_dot\"], n_bins[\"theta_dot\"]),\n",
        "        discretize(x, *state_ranges[\"x\"], n_bins[\"x\"]),\n",
        "        discretize(x_dot, *state_ranges[\"x_dot\"], n_bins[\"x_dot\"]),\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zxRb39B7_Xss"
      },
      "outputs": [],
      "source": [
        "# ========== Q-table initialization ========== #\n",
        "\n",
        "Q = np.zeros((\n",
        "    n_bins[\"sin_theta\"],\n",
        "    n_bins[\"cos_theta\"],\n",
        "    n_bins[\"theta_dot\"],\n",
        "    n_bins[\"x\"],\n",
        "    n_bins[\"x_dot\"],\n",
        "    N_ACTIONS\n",
        "), dtype=np.float32)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WEG-rpxKAD3N"
      },
      "outputs": [],
      "source": [
        "# ========== Decay Law for ε ========== #\n",
        "def epsilon_schedule(n, d):\n",
        "    return max(\n",
        "        epsilon_min,\n",
        "        min(1.0, 1.0 - np.log10(n + 1) / d)\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IdRx9AmtAOYp"
      },
      "outputs": [],
      "source": [
        "# ========== Greedy action selection ========== #\n",
        "def select_action(state_idx, epsilon):\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.randint(N_ACTIONS)\n",
        "    return np.argmax(Q[state_idx])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fVfjSC7nATwl"
      },
      "outputs": [],
      "source": [
        "# ========== Q-learning update rule ========== #\n",
        "def update_q(state, action, reward, next_state):\n",
        "    best_next = np.max(Q[next_state])\n",
        "    Q[state + (action,)] += alpha * (\n",
        "        reward + gamma * best_next - Q[state + (action,)]\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Ip6NzxzBEl2"
      },
      "outputs": [],
      "source": [
        "# ========== moving average function (check paper) ========== #\n",
        "def moving_average(data, window):\n",
        "    if len(data) < window:\n",
        "        return None\n",
        "    return float(np.mean(data[-window:]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_q_policy(env, Q, n_eval=10, return_best_traj=False):\n",
        "    returns = []\n",
        "\n",
        "    best_return = -np.inf\n",
        "    best_cos_traj = None\n",
        "\n",
        "    for _ in range(n_eval):\n",
        "        obs = env.reset_downward()\n",
        "        state = discretize_state(obs)\n",
        "\n",
        "        total_reward = 0.0\n",
        "        step_count = 0\n",
        "        done = False\n",
        "\n",
        "        cos_traj = []\n",
        "\n",
        "        while not done and step_count < max_steps:\n",
        "            cos_traj.append(np.cos(env.theta))\n",
        "\n",
        "            action = np.argmax(Q[state])   # greedy\n",
        "            U = actions[action]\n",
        "\n",
        "            obs = env.step(U)\n",
        "            reward = compute_reward(env.theta, env.x)\n",
        "\n",
        "            done, terminal_penalty = check_termination(\n",
        "                env.x, env.theta_dot, step_count\n",
        "            )\n",
        "            reward += terminal_penalty\n",
        "\n",
        "            total_reward += reward\n",
        "            state = discretize_state(obs)\n",
        "            step_count += 1\n",
        "\n",
        "        norm_ret = total_reward / max_steps\n",
        "        returns.append(norm_ret)\n",
        "\n",
        "        if return_best_traj and norm_ret > best_return:\n",
        "            best_return = norm_ret\n",
        "            best_cos_traj = cos_traj\n",
        "\n",
        "    mean_ret = float(np.mean(returns))\n",
        "    std_ret  = float(np.std(returns))\n",
        "\n",
        "    if return_best_traj:\n",
        "        return mean_ret, std_ret, best_cos_traj\n",
        "    else:\n",
        "        return mean_ret, std_ret\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_q_learning(env, N_T):\n",
        "    global_step = 0\n",
        "    d = N_T / 10   # epsilon decay parameter\n",
        "\n",
        "    best_eval_return = -np.inf\n",
        "    best_cos_traj = None\n",
        "\n",
        "    episode_returns = []\n",
        "    step_returns = []\n",
        "    eval_history = []   # (global_step, mean_return, std)\n",
        "\n",
        "    for episode in range(N_T):\n",
        "        obs = env.reset()        # upright reset (training)\n",
        "        state = discretize_state(obs)\n",
        "\n",
        "        epsilon = epsilon_schedule(episode, d)\n",
        "        total_reward = 0.0\n",
        "        done = False\n",
        "        step_count = 0\n",
        "\n",
        "        # ===== TRAINING EPISODE =====\n",
        "        while not done and step_count < max_steps:\n",
        "            action_idx = select_action(state, epsilon)\n",
        "            U = actions[action_idx]\n",
        "\n",
        "            obs = env.step(U)\n",
        "            reward = compute_reward(env.theta, env.x)\n",
        "\n",
        "            done, terminal_penalty = check_termination(\n",
        "                env.x, env.theta_dot, step_count\n",
        "            )\n",
        "            reward += terminal_penalty\n",
        "            total_reward += reward\n",
        "\n",
        "            next_state = discretize_state(obs)\n",
        "            update_q(state, action_idx, reward, next_state)\n",
        "\n",
        "            state = next_state\n",
        "            step_count += 1\n",
        "            global_step += 1\n",
        "\n",
        "            # ===== PAPER EVALUATION =====\n",
        "            if global_step % 5000 == 0:\n",
        "                mean_ret, std_ret, cos_traj = evaluate_q_policy(\n",
        "                    env, Q, n_eval=10, return_best_traj=True\n",
        "                )\n",
        "\n",
        "                eval_history.append((global_step, mean_ret, std_ret))\n",
        "\n",
        "                if mean_ret > best_eval_return:\n",
        "                    best_eval_return = mean_ret\n",
        "                    best_cos_traj = cos_traj\n",
        "\n",
        "        # ----- training diagnostics -----\n",
        "        norm_ret = total_reward / max_steps\n",
        "        episode_returns.append(norm_ret)\n",
        "        step_returns.append((global_step, norm_ret))\n",
        "\n",
        "        if episode % 1000 == 0 and episode > 0:\n",
        "            ma300 = moving_average(episode_returns, 300)\n",
        "            ma_str = f\"{ma300:.3f}\" if ma300 is not None else \"N/A\"\n",
        "            print(\n",
        "                f\"Episode {episode:6d}/{N_T} | \"\n",
        "                f\"Steps {global_step:9d} | \"\n",
        "                f\"ε={epsilon:.4f} | \"\n",
        "                f\"Return={norm_ret:.3f} | \"\n",
        "                f\"MA(300)={ma_str}\"\n",
        "            )\n",
        "\n",
        "    return step_returns, episode_returns, eval_history, best_cos_traj\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G685akXlw3-z"
      },
      "outputs": [],
      "source": [
        "# ========== Plot Normalized return vs time steps ========== #\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_learning_curve(step_returns, title, window=300):\n",
        "    steps = [s for s, r in step_returns]\n",
        "    returns = [r for s, r in step_returns]\n",
        "\n",
        "    plt.figure(figsize=(8, 4))\n",
        "\n",
        "    # raw returns\n",
        "    plt.plot(steps, returns, alpha=0.3, label=\"Normalized return\")\n",
        "\n",
        "    # moving average curve (array)\n",
        "    if len(returns) >= window:\n",
        "        ma_curve = np.convolve(\n",
        "            returns,\n",
        "            np.ones(window) / window,\n",
        "            mode=\"valid\"\n",
        "        )\n",
        "        ma_steps = steps[window - 1:]\n",
        "        plt.plot(ma_steps, ma_curve, linewidth=2, label=f\"MA({window})\")\n",
        "\n",
        "    plt.xlabel(\"Time steps\")\n",
        "    plt.ylabel(\"Normalized return\")\n",
        "    plt.title(title)\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yiKg7hePym7a"
      },
      "outputs": [],
      "source": [
        "# ========== Plot Normalized return vs time steps like the paper ========== #\n",
        "\n",
        "def plot_multiple_learning_curves(curves, window=300):\n",
        "\n",
        "    plt.figure(figsize=(8, 4))\n",
        "\n",
        "    for step_returns, label in curves:\n",
        "        steps = [s for s, r in step_returns]\n",
        "        returns = [r for s, r in step_returns]\n",
        "\n",
        "        # compute MA curve locally (array)\n",
        "        if len(returns) >= window:\n",
        "            ma_curve = np.convolve(\n",
        "                returns,\n",
        "                np.ones(window) / window,\n",
        "                mode=\"valid\"\n",
        "            )\n",
        "            ma_steps = steps[window - 1:]\n",
        "\n",
        "            plt.plot(\n",
        "                ma_steps,\n",
        "                ma_curve,\n",
        "                linewidth=2,\n",
        "                label=label\n",
        "            )\n",
        "\n",
        "    plt.xlabel(\"Time steps\")\n",
        "    plt.ylabel(\"Normalized return\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AS3wEhMTxdVV"
      },
      "outputs": [],
      "source": [
        "# ========== Plot cos(θ) vs time steps like the paper ========== #\n",
        "\n",
        "def plot_cos_theta_q(env, Q, steps=800):\n",
        "    obs = env.reset_downward()   # θ = 0\n",
        "    state = discretize_state(obs)\n",
        "\n",
        "    cos_vals = []\n",
        "\n",
        "    for t in range(steps):\n",
        "        cos_vals.append(np.cos(env.theta))\n",
        "\n",
        "        action = np.argmax(Q[state])   # greedy\n",
        "        U = actions[action]\n",
        "\n",
        "        obs = env.step(U)\n",
        "        state = discretize_state(obs)\n",
        "\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.plot(cos_vals)\n",
        "    plt.xlabel(\"Time steps\")\n",
        "    plt.ylabel(r\"$\\cos(\\theta)$\")\n",
        "    plt.title(\"Pendulum stabilization (one episode, 800 steps)\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_best_cos_theta(best_cos_traj):\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.plot(best_cos_traj)\n",
        "    plt.xlabel(\"Time steps\")\n",
        "    plt.ylabel(r\"$\\cos(\\theta)$\")\n",
        "    plt.title(\"Best greedy episode (800 steps)\")\n",
        "    plt.grid(True)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_q_eval_curve(eval_history, label):\n",
        "    steps = [s for s, _, _ in eval_history]\n",
        "    means = [m for _, m, _ in eval_history]\n",
        "\n",
        "    plt.plot(steps, means, label=label)\n",
        "    plt.xscale(\"log\")\n",
        "    plt.xlabel(\"Time step\")\n",
        "    plt.ylabel(\"Normalized return\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def smooth_curve(y, window=5):\n",
        "    if len(y) < window:\n",
        "        return y\n",
        "    return np.convolve(y, np.ones(window)/window, mode=\"valid\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_q_eval_curves(curves):\n",
        "    plt.figure(figsize=(6, 4))\n",
        "\n",
        "    for eval_hist, label in curves:\n",
        "        steps = np.array([s for s, _, _ in eval_hist])\n",
        "        means = np.array([m for _, m, _ in eval_hist])\n",
        "\n",
        "        smooth_means = smooth_curve(means, window=30)\n",
        "        smooth_steps = steps[len(steps) - len(smooth_means):]\n",
        "        plt.plot(smooth_steps, smooth_means, label=label)\n",
        "\n",
        "    plt.xscale(\"log\")\n",
        "    plt.xlabel(\"Time step\")\n",
        "    plt.ylabel(\"Normalized return\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "IR-IpldWyF9R",
        "outputId": "da931779-8427-43b3-f5c1-437d343d20f5"
      },
      "outputs": [],
      "source": [
        "# ========== Run and Plot ========== #\n",
        "env = CartPoleSimulator()\n",
        "\n",
        "_, _, eval_hist, cos_tr = train_q_learning(env, N_T=10000)\n",
        "\n",
        "# Paper-style learning curve\n",
        "plt.figure(figsize=(6,4))\n",
        "steps = [s for s, _, _ in eval_hist]\n",
        "means = [m for _, m, _ in eval_hist]\n",
        "plt.plot(steps, means)\n",
        "plt.xscale(\"log\")\n",
        "plt.xlabel(\"Time step\")\n",
        "plt.ylabel(\"Normalized return\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Paper-style cos(theta) plot\n",
        "plot_best_cos_theta(cos_tr)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 797
        },
        "id": "DowuKby60hEJ",
        "outputId": "dcd8030c-f132-4cf1-ba81-5503f31337e1"
      },
      "outputs": [],
      "source": [
        "env = CartPoleSimulator()\n",
        "\n",
        "_, _, eval_hist5, cos_tr = train_q_learning(env, N_T=100000)\n",
        "\n",
        "# Paper-style learning curve\n",
        "plt.figure(figsize=(6,4))\n",
        "steps = [s for s, _, _ in eval_hist5]\n",
        "means = [m for _, m, _ in eval_hist5]\n",
        "plt.plot(steps, means)\n",
        "plt.xscale(\"log\")\n",
        "plt.xlabel(\"Time step\")\n",
        "plt.ylabel(\"Normalized return\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Paper-style cos(theta) plot\n",
        "# plot_cos_theta_q(env, Q)\n",
        "plot_best_cos_theta(cos_tr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 814
        },
        "id": "KVn4STi-0hmF",
        "outputId": "d91ee5d0-7abd-4018-cb5e-1d5a95094270"
      },
      "outputs": [],
      "source": [
        "env = CartPoleSimulator()\n",
        "\n",
        "_, _, eval_hist6, cos_tr = train_q_learning(env, N_T=1000000)\n",
        "\n",
        "# Paper-style learning curve\n",
        "plt.figure(figsize=(6,4))\n",
        "steps = [s for s, _, _ in eval_hist6]\n",
        "means = [m for _, m, _ in eval_hist6]\n",
        "plt.plot(steps, means)\n",
        "plt.xscale(\"log\")\n",
        "plt.xlabel(\"Time step\")\n",
        "plt.ylabel(\"Normalized return\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Paper-style cos(theta) plot\n",
        "plot_best_cos_theta(cos_tr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 407
        },
        "id": "JRFvFxHVziIh",
        "outputId": "794cff7f-08f2-4fb8-b7aa-29b0064c1b16"
      },
      "outputs": [],
      "source": [
        "curves = [\n",
        "    (eval_hist,  r\"Q-learning ($N_T = 10^4$)\"),\n",
        "    (eval_hist5, r\"Q-learning ($N_T = 10^5$)\"),\n",
        "    (eval_hist6, r\"Q-learning ($N_T = 10^6$)\")\n",
        "]\n",
        "\n",
        "plot_q_eval_curves(curves)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "env = CartPoleSimulator()\n",
        "\n",
        "_, _, eval_hist67, cos_tr2 = train_q_learning(env, N_T=1000000)\n",
        "\n",
        "# Paper-style learning curve\n",
        "plt.figure(figsize=(6,4))\n",
        "steps = [s for s, _, _ in eval_hist67]\n",
        "means = [m for _, m, _ in eval_hist67]\n",
        "plt.plot(steps, means)\n",
        "plt.xscale(\"log\")\n",
        "plt.xlabel(\"Time step\")\n",
        "plt.ylabel(\"Normalized return\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Paper-style cos(theta) plot\n",
        "# plot_cos_theta_q(env, Q)\n",
        "plot_best_cos_theta(cos_tr2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_q_eval_curves(curves)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "curves = [\n",
        "    (eval_hist,  r\"Q-learning ($N_T = 10^4$)\"),\n",
        "    (eval_hist5, r\"Q-learning ($N_T = 10^5$)\"),\n",
        "    (eval_hist67, r\"Q-learning ($N_T = 10^6$)\")\n",
        "]\n",
        "\n",
        "plot_q_eval_curves(curves)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tXDF1nODGcrS"
      },
      "source": [
        "## Step 4: DQN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5UCj4cEeGe29"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import deque\n",
        "import random\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BU01f6vwHnSW"
      },
      "outputs": [],
      "source": [
        "# ========== DQN hyperparameters ========== #\n",
        "LR = 3e-4\n",
        "GAMMA = 0.995\n",
        "EPSILON = 0.178\n",
        "BUFFER_SIZE = 50_000\n",
        "BATCH_SIZE = 1024\n",
        "TARGET_UPDATE_INTERVAL = 1000  # time steps\n",
        "EVAL_EVERY_STEPS = 5000        # time steps\n",
        "MAX_GLOBAL_STEPS = 150_000   # total environment steps (paper-style)\n",
        "\n",
        "U0 = 2.4                 # [V] constant voltage magnitude\n",
        "\n",
        "actions = np.array([-U0, 0.0, U0])\n",
        "N_ACTIONS = len(actions)\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9PqENzDfJYhE"
      },
      "outputs": [],
      "source": [
        "def reset_with_ic(env, theta0, theta_dot0=0.0, x0_=0.0, x_dot0=0.0):\n",
        "  # function to set the true internal state and return the initial observation\n",
        "  env.theta = theta0\n",
        "  env.theta_dot = theta_dot0\n",
        "  env.x = x0\n",
        "  env.x_dot = x_dot0\n",
        "\n",
        "  return env._make_observation()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LVM-AjZcHXgu"
      },
      "outputs": [],
      "source": [
        "# ========== Network to use ========== #\n",
        "\n",
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, input_dim=5, hidden_dim=256, output_dim=3):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(hidden_dim, output_dim)  # linear output for Q-values\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZ55fgLYHf3N"
      },
      "outputs": [],
      "source": [
        "# ========== Replay Buffer with 50000 transitions ========== #\n",
        "\n",
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity=50_000):\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def push(self, s, a, r, s2, done):\n",
        "        # store as numpy arrays / python primitives\n",
        "        self.buffer.append((s, a, r, s2, done))\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        batch = random.sample(self.buffer, batch_size)\n",
        "        s, a, r, s2, done = map(np.array, zip(*batch))\n",
        "\n",
        "        # Convert to torch tensors\n",
        "        s = torch.tensor(s, dtype=torch.float32)\n",
        "        a = torch.tensor(a, dtype=torch.int64).unsqueeze(1)     # (B,1)\n",
        "        r = torch.tensor(r, dtype=torch.float32).unsqueeze(1)   # (B,1)\n",
        "        s2 = torch.tensor(s2, dtype=torch.float32)\n",
        "        done = torch.tensor(done, dtype=torch.float32).unsqueeze(1)  # (B,1)\n",
        "        return s, a, r, s2, done\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9JoXN5XHtfl"
      },
      "outputs": [],
      "source": [
        "# ========== Action Mapping ========== #\n",
        "\n",
        "actions = np.array([-U0, 0.0, U0], dtype=np.float32)\n",
        "\n",
        "def select_action_dqn(q_net, obs, epsilon=EPSILON):\n",
        "    # obs: numpy array shape (5,)\n",
        "    if np.random.rand() < epsilon:\n",
        "        return np.random.randint(3)\n",
        "    with torch.no_grad():\n",
        "        x = torch.tensor(obs, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "        q = q_net(x)  # (1,3)\n",
        "        return int(torch.argmax(q, dim=1).item())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nz3KMidXH09C"
      },
      "outputs": [],
      "source": [
        "# ========== One DQN step ========== #\n",
        "\n",
        "huber = nn.SmoothL1Loss()  # Huber loss (PyTorch SmoothL1Loss)\n",
        "\n",
        "def train_step(q_net, target_net, optimizer, replay_buffer):\n",
        "    if len(replay_buffer) < BATCH_SIZE:\n",
        "        return None\n",
        "\n",
        "    s, a, r, s2, done = replay_buffer.sample(BATCH_SIZE)\n",
        "    s = s.to(device); a = a.to(device); r = r.to(device); s2 = s2.to(device); done = done.to(device)\n",
        "\n",
        "    # Q(s,a) from online network\n",
        "    q_values = q_net(s).gather(1, a)  # (B,1)\n",
        "\n",
        "    # target: r + gamma * (1-done) * max_a' Q_target(s',a')\n",
        "    with torch.no_grad():\n",
        "        q_next = target_net(s2).max(dim=1, keepdim=True)[0]  # (B,1)\n",
        "        y = r + GAMMA * (1.0 - done) * q_next\n",
        "\n",
        "    loss = huber(q_values, y)\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    return float(loss.item())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ayGmIREAH_DR"
      },
      "outputs": [],
      "source": [
        "# ========== Evaluation Routine ========== #\n",
        "\n",
        "def evaluate_policy(env, q_net, n_eval=10):\n",
        "    q_net.eval()\n",
        "    returns = []\n",
        "\n",
        "    for _ in range(n_eval):\n",
        "        theta0 = np.deg2rad(np.random.uniform(-10.0, 10.0))\n",
        "        obs = reset_with_ic(env, theta0, 0.0, 0.0, 0.0)\n",
        "\n",
        "        total = 0.0\n",
        "        done = False\n",
        "        step_count = 0\n",
        "\n",
        "        while not done:\n",
        "            # greedy action\n",
        "            with torch.no_grad():\n",
        "                x = torch.tensor(obs, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "                a_idx = int(torch.argmax(q_net(x), dim=1).item())\n",
        "            U = float(actions[a_idx])\n",
        "\n",
        "            obs2 = env.step(U)\n",
        "\n",
        "            r = compute_reward(env.theta, env.x)\n",
        "            done, terminal_penalty = check_termination(env.x, env.theta_dot, step_count)\n",
        "            r += terminal_penalty\n",
        "            total += r\n",
        "\n",
        "            obs = obs2\n",
        "            step_count += 1\n",
        "\n",
        "        returns.append(total / max_steps)  # normalized return\n",
        "\n",
        "    q_net.train()\n",
        "    return float(np.mean(returns)), float(np.std(returns))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ThO9kmzXIGWJ"
      },
      "outputs": [],
      "source": [
        "# ========== Full Training (STEP-LIMITED) ========== #\n",
        "\n",
        "def train_dqn(env):\n",
        "    q_net = QNetwork().to(device)\n",
        "    target_net = QNetwork().to(device)\n",
        "    target_net.load_state_dict(q_net.state_dict())\n",
        "    target_net.eval()\n",
        "\n",
        "    optimizer = optim.Adam(q_net.parameters(), lr=LR)\n",
        "    replay = ReplayBuffer(capacity=BUFFER_SIZE)\n",
        "\n",
        "    global_step = 0\n",
        "    eval_history = []          # (global_step, mean_norm_return, std)\n",
        "    episode_returns = []\n",
        "\n",
        "    ep = 0\n",
        "    while global_step < MAX_GLOBAL_STEPS:\n",
        "        obs = env.reset()\n",
        "        done = False\n",
        "        step_count = 0\n",
        "        ep_steps = 0\n",
        "        ep_return = 0.0\n",
        "\n",
        "        # ----- Run one episode (collect transitions) -----\n",
        "        while not done and global_step < MAX_GLOBAL_STEPS:\n",
        "            a_idx = select_action_dqn(q_net, obs, epsilon=EPSILON)\n",
        "            U = float(actions[a_idx])\n",
        "\n",
        "            obs2 = env.step(U)\n",
        "\n",
        "            r = compute_reward(env.theta, env.x)\n",
        "            done, terminal_penalty = check_termination(\n",
        "                env.x, env.theta_dot, step_count\n",
        "            )\n",
        "            r += terminal_penalty\n",
        "\n",
        "            replay.push(obs, a_idx, r, obs2, done)\n",
        "\n",
        "            obs = obs2\n",
        "            step_count += 1\n",
        "            ep_steps += 1\n",
        "            global_step += 1\n",
        "            ep_return += r\n",
        "\n",
        "            # ----- Target network update -----\n",
        "            if global_step % TARGET_UPDATE_INTERVAL == 0:\n",
        "                target_net.load_state_dict(q_net.state_dict())\n",
        "\n",
        "            # ----- Evaluation -----\n",
        "            if global_step % EVAL_EVERY_STEPS == 0:\n",
        "                mean_ret, std_ret = evaluate_policy(env, q_net, n_eval=10)\n",
        "                eval_history.append((global_step, mean_ret, std_ret))\n",
        "                print(\n",
        "                    f\"[Eval @ step {global_step}] \"\n",
        "                    f\"mean norm return = {mean_ret:.3f} ± {std_ret:.3f}\"\n",
        "                )\n",
        "\n",
        "        # ----- Training after episode -----\n",
        "        losses = []\n",
        "        for _ in range(ep_steps):\n",
        "            loss = train_step(q_net, target_net, optimizer, replay)\n",
        "            if loss is not None:\n",
        "                losses.append(loss)\n",
        "\n",
        "        # ----- Normalize & store return -----\n",
        "        norm_ret = ep_return / max_steps\n",
        "        episode_returns.append(norm_ret)\n",
        "\n",
        "        # ----- Logging with MA(30) -----\n",
        "        if ep % 50 == 0:\n",
        "            ma30 = moving_average(episode_returns, window=30)\n",
        "            ma30_str = f\"{ma30:.3f}\" if ma30 is not None else \"N/A\"\n",
        "            avg_loss = float(np.mean(losses)) if losses else None\n",
        "\n",
        "            print(\n",
        "                f\"Episode {ep:6d} | \"\n",
        "                f\"steps={ep_steps:4d} | \"\n",
        "                f\"norm return={norm_ret:.3f} | \"\n",
        "                f\"MA(30)={ma30_str} | \"\n",
        "                f\"loss={avg_loss}\"\n",
        "            )\n",
        "\n",
        "        ep += 1\n",
        "\n",
        "    print(f\"Training stopped at global_step = {global_step}, episodes = {ep}\")\n",
        "    return q_net, eval_history\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def run_dqn_inference(env, q_net, steps=800):\n",
        "    obs = env.reset_downward()   # same IC as paper\n",
        "    \n",
        "    traj = {\n",
        "        \"theta\": [],\n",
        "        \"theta_dot\": [],\n",
        "        \"x\": [],\n",
        "        \"x_dot\": [],\n",
        "        \"U\": []\n",
        "    }\n",
        "\n",
        "    for t in range(steps):\n",
        "        # Log state\n",
        "        traj[\"theta\"].append(env.theta)\n",
        "        traj[\"theta_dot\"].append(env.theta_dot)\n",
        "        traj[\"x\"].append(env.x)\n",
        "        traj[\"x_dot\"].append(env.x_dot)\n",
        "\n",
        "        # Greedy action\n",
        "        with torch.no_grad():\n",
        "            x = torch.tensor(obs, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "            q_vals = q_net(x)\n",
        "            action_idx = torch.argmax(q_vals).item()\n",
        "\n",
        "        U = float(actions[action_idx])\n",
        "        traj[\"U\"].append(U)\n",
        "\n",
        "        obs = env.step(U)\n",
        "\n",
        "    return traj\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_x_time(traj, label):\n",
        "    plt.plot(traj[\"x\"], label=label)\n",
        "    plt.xlabel(\"Time step\")\n",
        "    plt.ylabel(\"x [m]\")\n",
        "    plt.grid(True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_theta_time(traj, label):\n",
        "    plt.plot(traj[\"theta\"], label=label)\n",
        "    plt.xlabel(\"Time step\")\n",
        "    plt.ylabel(r\"$\\theta$ [rad]\")\n",
        "    plt.grid(True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_cart_phase(traj, label):\n",
        "    plt.plot(traj[\"x\"], traj[\"x_dot\"], label=label)\n",
        "    plt.xlabel(\"x [m]\")\n",
        "    plt.ylabel(r\"$\\dot{x}$ [m/s]\")\n",
        "    plt.grid(True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_pendulum_phase(traj, label):\n",
        "    plt.plot(traj[\"theta\"], traj[\"theta_dot\"], label=label)\n",
        "    plt.xlabel(r\"$\\theta$ [rad]\")\n",
        "    plt.ylabel(r\"$\\dot{\\theta}$ [rad/s]\")\n",
        "    plt.grid(True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_voltage(traj, label, steps=200):\n",
        "    plt.step(range(steps), traj[\"U\"][:steps], where=\"post\", label=label)\n",
        "    plt.xlabel(\"Time step\")\n",
        "    plt.ylabel(\"Applied voltage [V]\")\n",
        "    plt.grid(True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7lIZ85lISBb",
        "outputId": "262c56eb-3495-4d47-9ed4-a216cbd28ae9"
      },
      "outputs": [],
      "source": [
        "env = CartPoleSimulator()\n",
        "q_net, eval_hist = train_dqn(env)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "steps = [s for s, _, _ in eval_hist]\n",
        "means = [m for _, m, _ in eval_hist]\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(steps, means)\n",
        "plt.xscale(\"log\")\n",
        "plt.xlabel(\"Time step\")\n",
        "plt.ylabel(\"Normalized return\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "traj = run_dqn_inference(env, q_net, steps=800)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "env2 = CartPoleSimulator()\n",
        "q_net2, eval_hist2 = train_dqn(env2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "traj2 = run_dqn_inference(env2, q_net2, steps=800)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_x_time(traj, \"DQN\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_cart_phase(traj, \"DQN\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_theta_time(traj, \"DQN\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_pendulum_phase(traj, \"DQN\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_voltage(traj, \"DQN\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "steps = [s for s, _, _ in eval_hist2]\n",
        "means = [m for _, m, _ in eval_hist2]\n",
        "\n",
        "plt.figure(figsize=(6,4))\n",
        "plt.plot(steps, means)\n",
        "plt.xscale(\"log\")\n",
        "plt.xlabel(\"Time step\")\n",
        "plt.ylabel(\"Normalized return\")\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_x_time_two(traj_green, traj_blue, label_green=\"7.1 V\", label_blue=\"2.4 V\"):\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.plot(traj_green[\"x\"], color=\"green\", label=label_green)\n",
        "    plt.plot(traj_blue[\"x\"], color=\"blue\", label=label_blue)\n",
        "    plt.xlabel(\"Time step\")\n",
        "    plt.ylabel(\"x [m]\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_cart_phase_two(traj_green, traj_blue, label_green=\"7.1 V\", label_blue=\"2.4 V\"):\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.plot(traj_green[\"x\"], traj_green[\"x_dot\"], color=\"green\", label=label_green)\n",
        "    plt.plot(traj_blue[\"x\"], traj_blue[\"x_dot\"], color=\"blue\", label=label_blue)\n",
        "    plt.xlabel(\"x [m]\")\n",
        "    plt.ylabel(r\"$\\dot{x}$ [m/s]\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_theta_time_two(traj_green, traj_blue, label_green=\"7.1 V\", label_blue=\"2.4 V\"):\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.plot(traj_green[\"theta\"], color=\"green\", label=label_green)\n",
        "    plt.plot(traj_blue[\"theta\"], color=\"blue\", label=label_blue)\n",
        "    plt.xlabel(\"Time step\")\n",
        "    plt.ylabel(r\"$\\theta$ [rad]\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_pendulum_phase_two(traj_green, traj_blue, label_green=\"7.1 V\", label_blue=\"2.4 V\"):\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.plot(traj_green[\"theta\"], traj_green[\"theta_dot\"], color=\"green\", label=label_green)\n",
        "    plt.plot(traj_blue[\"theta\"], traj_blue[\"theta_dot\"], color=\"blue\", label=label_blue)\n",
        "    plt.xlabel(r\"$\\theta$ [rad]\")\n",
        "    plt.ylabel(r\"$\\dot{\\theta}$ [rad/s]\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def plot_voltage_two(traj_green, traj_blue, steps=200, label_green=\"7.1 V\", label_blue=\"2.4 V\"):\n",
        "    plt.figure(figsize=(6,4))\n",
        "    plt.step(range(steps), traj_green[\"U\"][:steps], where=\"post\",\n",
        "             color=\"green\", label=label_green)\n",
        "    plt.step(range(steps), traj_blue[\"U\"][:steps], where=\"post\",\n",
        "             color=\"blue\", label=label_blue)\n",
        "    plt.xlabel(\"Time step\")\n",
        "    plt.ylabel(\"Applied voltage [V]\")\n",
        "    plt.grid(True)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_x_time_two(traj, traj2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_cart_phase_two(traj, traj2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_theta_time_two(traj, traj2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_pendulum_phase_two(traj, traj2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plot_voltage_two(traj, traj2)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python3.8 (slp)",
      "language": "python",
      "name": "slp"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.20"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
